# -*- coding: utf-8 -*-
"""week1-agisha.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K1sVrzA-59sKdVcYjrUcpPTMjJ-Nb6QK
"""

from google.colab import drive
drive.mount('/content/gdrive')

root_path = 'gdrive/My Drive/Google-Colab/AI-Mentorship/Kaggle/'

"""# Import Library"""

import numpy as np
import pandas as pd
import seaborn as sns

df_train = pd.read_csv('gdrive/My Drive/Google-Colab/AI-Mentorship/Kaggle/train.csv')
df_test = pd.read_csv('gdrive/My Drive/Google-Colab/AI-Mentorship/Kaggle/test.csv')

"""# Data Cleaning"""

def check_null(data):
    null_val = data.isnull().sum()
    null_val = null_val[null_val > 0 ]
    print(null_val)

check_null(df_train)

check_null(df_test)

"""# Fill nan values"""

df_train.Age = df_train.Age.fillna(df_train.Age.mean())
df_test.Age = df_test.Age.fillna(df_train.Age.mean())
df_test.Fare = df_test.Fare.fillna(df_train.Fare.median())
df_train.Embarked = df_train.Embarked.fillna(df_train.Embarked.mode()[0])

"""# #Dropping Coloumn"""

drop_column = ['PassengerId','Cabin', 'Ticket','Fare']
df_train.drop(drop_column, axis=1, inplace = True)
df_test.drop(drop_column, axis=1, inplace = True)

check_null(df_train)

check_null(df_test)

df_train.head()

"""# Feature Engineering"""

data = [df_train, df_test]

for dataset in data:
     dataset['Age'] = df_train['Age'].astype(int)
     
     genders = {"male": 0, "female": 1}
     dataset['Sex'] = dataset['Sex'].map(genders)
        
     ports = {"S": 0, "C": 1, "Q": 2}
     dataset['Embarked'] = dataset['Embarked'].map(ports)

print(df_train.isnull().sum())
print(df_test.isnull().sum())

#new features 
for dataset in data:    
    dataset['Family'] = dataset ['SibSp'] + dataset['Parch'] + 1
    dataset['Check Alone'] = 1 #initialize to yes/1 is alone
    dataset['Check Alone'].loc[dataset['Family'] > 1] = 0 # now update to no/0 if family size is greater than 1c
    dataset['Check Alone'] = dataset['Check Alone'].astype(int)

print(df_train.isnull().sum())
    print(df_test.isnull().sum())

df_train['Check Alone'].value_counts()

#grouping name'
titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}
for dataset in data:
    # extracting first
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
    # replace titles with a more common title or as Rare
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\
                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    # convert titles into numbers
    dataset['Title'] = dataset['Title'].map(titles)
    # filling NaN with 0, to get safe
    dataset['Title'] = dataset['Title'].fillna(0)

df_train = df_train.drop(['Name'], axis=1)
df_test = df_test.drop(['Name'], axis=1)

#grouping age
data = [df_train, df_test]
for dataset in data:
    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0
    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1
    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2
    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3
    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4
    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5
    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6
    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6
    
    dataset['Age'] = dataset['Age'].astype(int)

df_train.head() #agenya blm ke grouping :(

print('train dataset: %s, test dataset %s' %(str(df_train.shape), str(df_test.shape)) )

df_train.head()

"""# Split Data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train.drop('Survived',axis=1), 
                                                    df_train['Survived'], test_size=0.30, 
                                                    random_state=101)



"""# Training and Validating"""

from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)
predictions = logmodel.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

